set(['1/4=76\n', '[I:setbarnb 80]', 'G2>', 'G2<', '1/2=56\n', '1/2=60\n', '[I:setbarnb 61]', '[K:F]', '[I:setbarnb 91]', '[I:setbarnb 41]', 'G7', 'G6', 'G4', 'G3', 'G2', '[I:setbarnb 48]', 'G>', 'G<', '[I:setbarnb 56]', 'G8', '1/2=100\n', 'G/', 'G,', 'g7', 'g6', 'g4', 'g3', 'g2', 'g>', 'g<', 'F2', 'g8', "g'", '_A4', 'F4>', '1/4=152\n', 'g/', '16/2\n', "^c'", '_A2', '^A2', 'B2>', 'B2<', '^A6', 'G,4<', '^A4', 'G,4>', 'F clef=treble\n', '^A>', '8/4\n', '^A<', 'c4<', '_e2>', '[I:setbarnb 58]', '_e2<', '_A>', '3/8=60\n', 'e/8', 'e/>', '3/4=60\n', 'e/<', '1/4=100\n', 'b2>', 'b2<', '^a6', '^c>', '^a4', 'A,2>', 'A2<', 'e4<', 'n', 'd4', 'M:', '1/4=84\n', '^E/', 'g2>', '^c4', 'g2<', 'E,2>', 'Db clef=treble\n', 'F2>', '3/8=144\n', '[I:setbarnb 34]', '[K:G]', 'Z4', 'Z5', 'A clef=treble\n', '3/8=132\n', 'Z2', 'Z3', '_B>', 'z/', '[K:F#]', '3/4=63\n', '^d6', 'G,/', '^c7', '[I:setbarnb 5]', 'z<', '1/2=88\n', 'z>', 'Eb clef=treble\n', 'z8', '[I:setbarnb 24]', 'G,>', 'z4', '1/2=54\n', 'G,2', 'G,3', 'G,4', 'z2', 'z3', 'A,/', '_G/', 'F7', 'g4<', 'E clef=treble\n', '[I:setbarnb 66]', 'A,>', '[M:3/4]', '6/8\n', 'A,8', 'A,6', 'A,4', 'A,2', 'A,3', '3/2\n', '_G2', "d'2<", "d'2>", "d'4>", '12/2\n', 'A4>', 'B,>', '1/4=96\n', "e'2>", 'G/<', 'e7', '3/8=184\n', '_g2', "c'2<", '6/2\n', '_d4', 'D clef=bass\n', 'C,2>', 'C,2<', 'F/<', "c'/>", '[I:setbarnb 40]', '_A2<', '5/8\n', 'F', 'E2<', '^e2', 'E2>', '^e3', '_A8', 'B,3', '[M:4/8]', 'F,2<', 'F,2>', 'F3', 'F4', 'F6', 'f', 'F8', 'c4>', '1/8=152\n', 'F<', '^E3', 'F>', 'F clef=bass\n', '[I:setbarnb 11]', '^G,2>', '3/8=126\n', 'F,', '1/4=168\n', 'F/', '^e>', 'B,<', 'f2', 'f3', 'f4', '[M:10/4]', 'f6', 'f7', 'f8', 'B,4', '^e4', 'B,6', 'f<', 'F4<', 'f>', 'B,2', '^e/', 'B,/', '^d<', "f'", '^D,4', '[I:setbarnb 78]', 'f/', '[I:setbarnb 30]', '[I:setbarnb 65]', '^B/', '^B,', 'x/>', 'z2>', 'z2<', '^C,2>', '1/4=88\n', '[M:6/2]', '^B3', '^B2', '^E4', '^B4', 'A', '[I:setbarnb 93]', 'w', '[M:8/4]', '_b2', 'G,<', '[I:setbarnb 44]', '^G2>', '^D,2', 'D4<', 'L:', 'a', '3/8=108\n', 'e2<', 'e2>', '[I:setbarnb 23]', '[M:5/8]', '[K:Bb]', '^C4', '^E2', '[I:setbarnb 9]', '[I:setbarnb 71]', '_a2', '_G', '_E', '_D', '_B', '_A', '[I:setbarnb 68]', '^c4<', '^c4>', 'D,<', 'f4<', '[M:7/4]', 'f4>', '[I:setbarnb 116]', '3/8=80\n', '_g', 'A/>', '_e', '_d', '_c', '_b', '_a', 'A/8', '^G,4', '^A3', '1/2=63\n', 'D clef=treble\n', '3/8=96\n', '2/4\n', 'D/>', 'a/>', '[I:setbarnb 134]', 'a/<', "e'>", 'A,4>', "e'3", "e'2", '_E,', 'l', '1/8=120\n', '[M:2/2]', 'A,4<', "e'/", 'd4>', 'd4<', "^c'4", '|', 'C,/', 'b<', '[I:setbarnb 4]', '^A,>', '[M:1/4]', '1/8=168\n', '[I:setbarnb 57]', 'C,8', 'C,4', '[K:Ab]', 'C,6', 'G4<', 'C,2', '1/4=92\n', 'G clef=treble\n', '^F6', 'E/', '^F4', '^F3', '^F2', 'p', '5/2\n', '^F>', '^F<', '_A,/>', '^F8', 'E<', 'E>', '3/8=168\n', 'E8', '3/8=63\n', '^F/', 'E4', 'E7', 'E6', '3/8=152\n', 'E3', 'E2', '^f6', 'e/', '^f4', '^f3', '^f2', 'G', '^f>', "e'", '^f<', "^f'", 'e<', 'e>', 'e8', 'x', '^f/', 'e4', 'A4<', 'e6', 'e3', 'e2', '_B,4', '1/2=120\n', '_B,2', 'g', 'G clef=bass\n', '^A/', 'G,2>', 'd/8', 'G,2<', 'd/>', '^A,', 'd/<', '[I:setbarnb 20]', '_B,/', '^D3', 'D4>', '1/4=126\n', '^a2', '^F,', '4/4\n', '^G2<', 'D,2>', 'D,2<', '1/2=152\n', '[I:setbarnb 26]', 'G/>', 'G/8', 'K:', "e'4", '[M:13/8]', 'A,2<', '^b', '[I:setbarnb 12]', '[M:2/1]', '1/2=48\n', '7/8\n', 'g4>', '^C,3', 'B', '^D/', '^C,6', '3/8=84\n', '[M:12/8]', '^C,>', 'x8', 'x>', '^e', 'x2', 'x3', 'x6', 'x7', 'x4', 'b', '[I:setbarnb 19]', "e'6", '3/4=69\n', '[I:setbarnb 29]', 'r', 'G,/>', 'a4>', 'a4<', '[I:setbarnb 35]', 'B/8', '^G,3', '^G,2', '1/4=176\n', 'Q:', 'D,/', '[K:D]', 'D,2', '^C,2', 'D,6', '1/4=160\n', 'D,4', 'D,8', 'D,>', 'x/', 'B/>', '[I:setbarnb 54]', '^d>', '^a/', 'g/<', 'g/>', 'g/8', '[I:setbarnb 8]', '[K:C]', "f'4", 'Bb clef=bass\n', '1/4\n', "f'3", '9/8\n', "f'/", '^A', '^B', '^C', ']', '^E', '^F', '^G', 'D/', 'D,', '[K:clef=bass]', '^C/', '^C,', 'B4<', 'B4>', 'm', 'D>', 'D<', '^C>', 'E,2<', 'D6', 'D7', '^C2', '^c', 'D2', '[I:setbarnb 18]', '^f', '^g', 'd/', '^c/', "d'", '3/8=240\n', 'd>', '3/8=69\n', 'd<', '^C2<', '^c<', 'd8', 'C<', 'd6', 'd7', '^c2', '^c3', 'd2', 'd3', '^c6', 'F2<', '[I:setbarnb 16]', '_B<', 'Z', '^C2>', '_B8', '_B6', '_B4', '_B2', '_B3', '_f/', '7/4\n', '_B/', '[I:setbarnb 36]', 'x2>', '^a', '[I:setbarnb 99]', '_b>', '_b<', '1/2=52\n', '[I:setbarnb 3]', '_b6', '_b4', 'C2>', '_b3', 'C2<', '_b/', '[I:setbarnb 22]', '^e/>', '[I:setbarnb 38]', '3/8=88\n', 'Z6', 'Z7', '_E/', '[I:setbarnb 31]', '[I:setbarnb 76]', '_E>', '_E3', '_E2', '_E6', '_E4', '_e/', '_E,>', "_e'", '_e8', '_e>', '_e<', '_e3', '_e2', '_e6', '_e4', '[M:10/8]', '_E,3', '[I:setbarnb 46]', '1/8=160\n', '^g2<', '^c2>', '^c2<', '^F,3', 'f2>', 'z/>', 'f2<', '^A/>', 'C', '2/2\n', 'E4>', '^F/>', '3/8=160\n', "d'/", 'E4<', 'C/>', "d'2", "d'3", "d'6", "d'4", "d'8", '[M:2/16]', "d'>", "d'<", '^G4', 'G,8', 'c', '^G2', '^G3', '^G>', 'c2>', 'c2<', "c'2>", '3/8=100\n', '_d2<', '[I:setbarnb 125]', '^F,2>', '^G,', '[M:2/4]', '^G/', '_a2<', '^g4', '1/2=80\n', '^g6', '^d/>', '^g2', '^g3', '^g>', '^g8', ',', 'G,6', '1/4=138\n', '[M:9/4]', '^g/', 'z6', '[I:setbarnb 73]', 'z7', 'C/', 'C,', '_B/>', '[I:setbarnb 28]', 'C8', 'C>', '^c/>', '8/8\n', 'C3', 'C2', 'C6', '[I:setbarnb 32]', 'C4', '^G,>', 'c/', '1/2=76\n', '[I:setbarnb 69]', '^F2>', '^F2<', "c'", '^G6', 'c8', 'c>', '[I:setbarnb 50]', 'c<', 'c3', 'c2', 'c7', 'c6', 'c4', 'D,3', '[M:1/2]', 'A/<', '_A3', '9/4\n', '_a2>', 'c/8', '[I:setbarnb 21]', '^f/>', 'e4>', 'c/<', 'A2>', 'c/>', '[I:setbarnb 7]', '1/4=120\n', '1/2=96\n', '_A/', '_A,', '[I:setbarnb 59]', '_a4', '_a3', '[I:setbarnb 43]', '_A2>', '_a>', 'F/>', 'a2<', 'a2>', '_A,/', '_e4>', '1/2=66\n', '[I:setbarnb 27]', 'A,<', 'D4', '[I:setbarnb 110]', '[I:setbarnb 53]', '[M:22/8]', 'A,/>', '3/8=176\n', '[M:3/16]', '3/8=112\n', '_G7', '1/16\n', '^f2>', '^f2<', 'B,/>', 'B clef=treble\n', '[I:setbarnb 13]', '^e2<', '^F,/', '3/8=116\n', '_E,/', '3/4=66\n', '[I:setbarnb 2]', '_A,4', '^F,>', '[M:9/8]', '[I:setbarnb 25]', '^F,4', '^F,6', '1/4=72\n', '^F,2', '[I:setbarnb 39]', 'Eb clef=bass\n', '[M:8/8]', '[I:setbarnb 67]', 'f/<', 'f/>', 'f/8', '[I:setbarnb 77]', '[M:6/4]', '_A,3', '[M:19/16]', '[I:setbarnb 49]', '_B,', 'D', '3/8=138\n', 'A6', '^g2>', '1/2=44\n', '[I:setbarnb 42]', 'E,8', '^e2>', 'F,/', '1/2=72\n', '_A,2', '3/8=72\n', '[M:3/2]', '14/2\n', 'd', 'B,', 'B/', 'B4', '1/4=132\n', 'B6', 'B7', 't', 'B2', 'B3', 'B<', 'B>', 'B8', 'E,/', '1/2=92\n', 'b/', '3/4\n', '4/2\n', 'b4', 'b6', 'E,>', '_b2<', 'b2', 'b3', 'E,2', 'E,3', 'b>', '12/8\n', 'E,6', 'E,4', '_D,', 'F,>', '1/8=144\n', '_a/', '[I:setbarnb 122]', '[I:setbarnb 10]', '1/8=176\n', '_D4', '[I:setbarnb 138]', '_B2<', '1/8=240\n', '_d/', '_B2>', 'F,4<', "f'2", '_d>', '_d2', '[M:5/4]', '1/4=69\n', '1/4=56\n', '[I:setbarnb 17]', 'B,8', '1/8\n', '_B,2>', 'o', '[I:setbarnb 15]', '[I:setbarnb 37]', '^A2>', '^C,4', '_A<', 'a4', '2/1\n', '[M:4/4]', '^c8', '3/8=120\n', '[M:7/8]', "^c'/", "^c'2", '1/4=60\n', '[I:setbarnb 51]', '[I:setbarnb 92]', '1/4=80\n', '[K:Eb]', '[M:6/8]', '5/4\n', '3/8=66\n', 'i', '^D>', '^D', '1/2=104\n', '^D2', '^D4', 'E/>', '[K:clef=treble]', '[I:setbarnb 6]', '2/8\n', '[M:4/2]', '^D2<', '^D2>', 'C clef=bass\n', 'B/<', 'D2<', "c'/", 'D2>', '[I:setbarnb 85]', '1/2=84\n', '^d3', '^d2', '^d4', '[K:A]', '1/4=144\n', "c'8", "c'<", '^d/', "c'>", 'b/<', "c'3", "c'2", 'C clef=treble\n', "c'4", "c'6", "^d'2", '3/8=92\n', '[I:setbarnb 45]', 'C,>', '1/4=112\n', '1/4=104\n', '3/8=76\n', 'z', "^d'/", '3/8\n', '[I:setbarnb 105]', 'E,<', 'Bb clef=treble\n', '_b2>', '1/8=112\n', '1/4=108\n', 'F/8', '1/4=192\n', 'B,2<', '[M:2/8]', 'B,2>', 'b8', "g'2", 'A,', 'A/', '10/2\n', '1/4=46\n', 'A3', 'A2', 'A4', 'A7', '6/4\n', 'A8', 'C,3', 'A<', '[I:setbarnb 14]', 'A>', '8/16\n', '[I:setbarnb 1]', 'E', 'D8', '[M:3/8]', 'E,', 'F,3', 'F,2', 'F,4', 'a/', 'F,6', 'a3', 'a2', '_E2>', '1/2=69\n', '_E2<', 'a6', 'a8', 'E/8', 'a<', 'G4>', 'a>', '^d2>', '^C3', 'e', 'd2<', 'b/>', 'd2>', '^d', '4/8\n', 'D3', '^G4>', 'u', "_e'2", 'Ab clef=treble\n', '[I:setbarnb 33]'])
vocabulary size: 895
n tunes: 18107
n train tunes: 17211
n validation tunes: 896
min, max length 15 1884
Building the model
  number of parameters: 3262848
  layer output shapes:               #params:   output shape:
    InputLayer                       0          (32, None)
    EmbeddingLayer                   801025     (32, None, 895)
    InputLayer                       0          (32, None)
    LSTMLayer                        1180160    (32, None, 256)
    DropoutLayer                     0          (32, None, 256)
    LSTMLayer                        525824     (32, None, 256)
    DropoutLayer                     0          (32, None, 256)
    LSTMLayer                        525824     (32, None, 256)
    DropoutLayer                     0          (32, None, 256)
    ReshapeLayer                     0          (None, 256)
    DenseLayer                       230015     (None, 895)
Train model
1/53700 (epoch 0.002) train_loss=3908.72460938 time/batch=1.46s
2/53700 (epoch 0.004) train_loss=3070.03369141 time/batch=0.56s
3/53700 (epoch 0.006) train_loss=253.74029541 time/batch=0.09s
4/53700 (epoch 0.007) train_loss=1589.26391602 time/batch=0.29s
5/53700 (epoch 0.009) train_loss=1446.82666016 time/batch=0.36s
6/53700 (epoch 0.011) train_loss=1419.64038086 time/batch=0.37s
7/53700 (epoch 0.013) train_loss=1017.16668701 time/batch=0.27s
8/53700 (epoch 0.015) train_loss=1106.59509277 time/batch=0.31s
9/53700 (epoch 0.017) train_loss=964.90045166 time/batch=0.26s
10/53700 (epoch 0.019) train_loss=289.51263428 time/batch=0.08s
11/53700 (epoch 0.020) train_loss=2158.57641602 time/batch=0.72s
12/53700 (epoch 0.022) train_loss=469.83105469 time/batch=0.18s
13/53700 (epoch 0.024) train_loss=585.36553955 time/batch=0.15s
14/53700 (epoch 0.026) train_loss=600.70007324 time/batch=0.16s
15/53700 (epoch 0.028) train_loss=312.51745605 time/batch=0.08s
16/53700 (epoch 0.030) train_loss=495.08215332 time/batch=0.13s
17/53700 (epoch 0.032) train_loss=808.19732666 time/batch=0.22s
18/53700 (epoch 0.034) train_loss=141.67541504 time/batch=0.05s
19/53700 (epoch 0.035) train_loss=1599.60742188 time/batch=0.39s
20/53700 (epoch 0.037) train_loss=759.47271729 time/batch=0.22s
21/53700 (epoch 0.039) train_loss=234.15061951 time/batch=0.07s
22/53700 (epoch 0.041) train_loss=358.92849731 time/batch=0.09s
23/53700 (epoch 0.043) train_loss=841.02478027 time/batch=0.23s
24/53700 (epoch 0.045) train_loss=961.93591309 time/batch=0.27s
25/53700 (epoch 0.047) train_loss=516.64184570 time/batch=0.15s
26/53700 (epoch 0.048) train_loss=1112.10131836 time/batch=0.31s
27/53700 (epoch 0.050) train_loss=536.46496582 time/batch=0.16s
28/53700 (epoch 0.052) train_loss=458.18389893 time/batch=0.12s
29/53700 (epoch 0.054) train_loss=185.79312134 time/batch=0.06s
30/53700 (epoch 0.056) train_loss=601.12744141 time/batch=0.15s
31/53700 (epoch 0.058) train_loss=776.70410156 time/batch=0.21s
32/53700 (epoch 0.060) train_loss=868.00775146 time/batch=0.24s
33/53700 (epoch 0.061) train_loss=712.18988037 time/batch=0.20s
34/53700 (epoch 0.063) train_loss=440.09625244 time/batch=0.12s
35/53700 (epoch 0.065) train_loss=1294.85644531 time/batch=0.34s
36/53700 (epoch 0.067) train_loss=1431.64697266 time/batch=0.41s
37/53700 (epoch 0.069) train_loss=100.63110352 time/batch=0.06s
38/53700 (epoch 0.071) train_loss=582.54656982 time/batch=0.15s
39/53700 (epoch 0.073) train_loss=910.91302490 time/batch=0.25s
40/53700 (epoch 0.074) train_loss=922.04748535 time/batch=0.25s
41/53700 (epoch 0.076) train_loss=1625.37890625 time/batch=0.84s
42/53700 (epoch 0.078) train_loss=560.74334717 time/batch=0.22s
43/53700 (epoch 0.080) train_loss=806.50964355 time/batch=0.21s
44/53700 (epoch 0.082) train_loss=665.78491211 time/batch=0.19s
45/53700 (epoch 0.084) train_loss=259.48687744 time/batch=0.08s
46/53700 (epoch 0.086) train_loss=269.52825928 time/batch=0.07s
47/53700 (epoch 0.088) train_loss=920.68603516 time/batch=0.25s
48/53700 (epoch 0.089) train_loss=3471.12402344 time/batch=1.92s
49/53700 (epoch 0.091) train_loss=129.73260498 time/batch=0.22s
50/53700 (epoch 0.093) train_loss=543.47027588 time/batch=0.13s
51/53700 (epoch 0.095) train_loss=1057.54907227 time/batch=0.31s
52/53700 (epoch 0.097) train_loss=282.51940918 time/batch=0.09s
53/53700 (epoch 0.099) train_loss=727.70465088 time/batch=0.20s
54/53700 (epoch 0.101) train_loss=846.75659180 time/batch=0.24s
55/53700 (epoch 0.102) train_loss=172.32301331 time/batch=0.06s
56/53700 (epoch 0.104) train_loss=390.38085938 time/batch=0.11s
57/53700 (epoch 0.106) train_loss=229.72402954 time/batch=0.08s
58/53700 (epoch 0.108) train_loss=504.72741699 time/batch=0.14s
59/53700 (epoch 0.110) train_loss=1055.79931641 time/batch=0.30s
60/53700 (epoch 0.112) train_loss=504.80667114 time/batch=0.16s
61/53700 (epoch 0.114) train_loss=800.26428223 time/batch=0.22s
62/53700 (epoch 0.115) train_loss=629.29998779 time/batch=0.18s
63/53700 (epoch 0.117) train_loss=735.89233398 time/batch=0.20s
64/53700 (epoch 0.119) train_loss=435.16311646 time/batch=0.13s
65/53700 (epoch 0.121) train_loss=126.44100189 time/batch=0.05s
66/53700 (epoch 0.123) train_loss=392.07580566 time/batch=0.11s
67/53700 (epoch 0.125) train_loss=573.54443359 time/batch=0.16s
68/53700 (epoch 0.127) train_loss=292.66217041 time/batch=0.09s
69/53700 (epoch 0.128) train_loss=200.04995728 time/batch=0.06s
70/53700 (epoch 0.130) train_loss=396.30938721 time/batch=0.11s
71/53700 (epoch 0.132) train_loss=526.92102051 time/batch=0.14s
72/53700 (epoch 0.134) train_loss=676.31347656 time/batch=0.19s
73/53700 (epoch 0.136) train_loss=307.47961426 time/batch=0.11s
74/53700 (epoch 0.138) train_loss=336.98333740 time/batch=0.09s
75/53700 (epoch 0.140) train_loss=969.01928711 time/batch=0.26s
76/53700 (epoch 0.142) train_loss=844.03002930 time/batch=0.26s
77/53700 (epoch 0.143) train_loss=820.75903320 time/batch=0.23s
78/53700 (epoch 0.145) train_loss=586.82562256 time/batch=0.17s
79/53700 (epoch 0.147) train_loss=331.88415527 time/batch=0.10s
80/53700 (epoch 0.149) train_loss=568.14550781 time/batch=0.16s
81/53700 (epoch 0.151) train_loss=696.31079102 time/batch=0.20s
82/53700 (epoch 0.153) train_loss=178.01663208 time/batch=0.06s
83/53700 (epoch 0.155) train_loss=128.99920654 time/batch=0.04s
84/53700 (epoch 0.156) train_loss=606.12536621 time/batch=0.16s
85/53700 (epoch 0.158) train_loss=663.52624512 time/batch=0.19s
86/53700 (epoch 0.160) train_loss=654.02819824 time/batch=0.18s
87/53700 (epoch 0.162) train_loss=449.30029297 time/batch=0.13s
88/53700 (epoch 0.164) train_loss=303.80004883 time/batch=0.09s
89/53700 (epoch 0.166) train_loss=617.15777588 time/batch=0.18s
90/53700 (epoch 0.168) train_loss=480.26794434 time/batch=0.12s
91/53700 (epoch 0.169) train_loss=606.07366943 time/batch=0.17s
92/53700 (epoch 0.171) train_loss=177.16128540 time/batch=0.06s
93/53700 (epoch 0.173) train_loss=749.11016846 time/batch=0.21s
94/53700 (epoch 0.175) train_loss=569.07080078 time/batch=0.17s
95/53700 (epoch 0.177) train_loss=482.26794434 time/batch=0.14s
96/53700 (epoch 0.179) train_loss=363.81683350 time/batch=0.10s
97/53700 (epoch 0.181) train_loss=511.08245850 time/batch=0.15s
98/53700 (epoch 0.182) train_loss=426.28689575 time/batch=0.12s
99/53700 (epoch 0.184) train_loss=434.08807373 time/batch=0.13s
100/53700 (epoch 0.186) train_loss=286.07153320 time/batch=0.09s
101/53700 (epoch 0.188) train_loss=669.17279053 time/batch=0.17s
102/53700 (epoch 0.190) train_loss=623.42980957 time/batch=0.18s
103/53700 (epoch 0.192) train_loss=221.43652344 time/batch=0.08s
104/53700 (epoch 0.194) train_loss=116.11343384 time/batch=0.04s
105/53700 (epoch 0.196) train_loss=491.90881348 time/batch=0.14s
106/53700 (epoch 0.197) train_loss=1041.54541016 time/batch=0.29s
107/53700 (epoch 0.199) train_loss=311.66616821 time/batch=0.11s
108/53700 (epoch 0.201) train_loss=169.08227539 time/batch=0.06s
109/53700 (epoch 0.203) train_loss=147.03884888 time/batch=0.04s
110/53700 (epoch 0.205) train_loss=327.42724609 time/batch=0.10s
111/53700 (epoch 0.207) train_loss=601.57299805 time/batch=0.16s
112/53700 (epoch 0.209) train_loss=449.68856812 time/batch=0.14s
113/53700 (epoch 0.210) train_loss=745.33020020 time/batch=0.21s
114/53700 (epoch 0.212) train_loss=222.57345581 time/batch=0.08s
115/53700 (epoch 0.214) train_loss=719.80371094 time/batch=0.19s
116/53700 (epoch 0.216) train_loss=190.70658875 time/batch=0.07s
117/53700 (epoch 0.218) train_loss=260.37515259 time/batch=0.07s
118/53700 (epoch 0.220) train_loss=502.30679321 time/batch=0.15s
119/53700 (epoch 0.222) train_loss=216.17817688 time/batch=0.06s
120/53700 (epoch 0.223) train_loss=203.29725647 time/batch=0.06s
121/53700 (epoch 0.225) train_loss=763.59704590 time/batch=0.20s
122/53700 (epoch 0.227) train_loss=705.54968262 time/batch=0.21s
123/53700 (epoch 0.229) train_loss=332.13809204 time/batch=0.10s
124/53700 (epoch 0.231) train_loss=433.57003784 time/batch=0.13s
125/53700 (epoch 0.233) train_loss=266.96011353 time/batch=0.08s
126/53700 (epoch 0.235) train_loss=381.67599487 time/batch=0.11s
127/53700 (epoch 0.236) train_loss=636.01733398 time/batch=0.18s
128/53700 (epoch 0.238) train_loss=444.61413574 time/batch=0.13s
129/53700 (epoch 0.240) train_loss=292.84066772 time/batch=0.09s
130/53700 (epoch 0.242) train_loss=120.23011780 time/batch=0.05s
131/53700 (epoch 0.244) train_loss=333.99877930 time/batch=0.11s
132/53700 (epoch 0.246) train_loss=510.47152710 time/batch=0.14s
133/53700 (epoch 0.248) train_loss=279.80615234 time/batch=0.09s
134/53700 (epoch 0.250) train_loss=165.11604309 time/batch=0.06s
135/53700 (epoch 0.251) train_loss=622.04095459 time/batch=0.16s
136/53700 (epoch 0.253) train_loss=317.65161133 time/batch=0.10s
137/53700 (epoch 0.255) train_loss=417.04895020 time/batch=0.12s
138/53700 (epoch 0.257) train_loss=596.67517090 time/batch=0.17s
139/53700 (epoch 0.259) train_loss=402.02563477 time/batch=0.12s
140/53700 (epoch 0.261) train_loss=726.88989258 time/batch=0.23s
141/53700 (epoch 0.263) train_loss=321.43548584 time/batch=0.10s
142/53700 (epoch 0.264) train_loss=627.49041748 time/batch=0.17s
143/53700 (epoch 0.266) train_loss=590.40093994 time/batch=0.18s
144/53700 (epoch 0.268) train_loss=152.10421753 time/batch=0.06s
145/53700 (epoch 0.270) train_loss=216.51599121 time/batch=0.07s
146/53700 (epoch 0.272) train_loss=524.59399414 time/batch=0.14s
147/53700 (epoch 0.274) train_loss=528.42791748 time/batch=0.16s
148/53700 (epoch 0.276) train_loss=369.27929688 time/batch=0.12s
149/53700 (epoch 0.277) train_loss=564.75952148 time/batch=0.16s
150/53700 (epoch 0.279) train_loss=465.22589111 time/batch=0.14s
151/53700 (epoch 0.281) train_loss=338.20928955 time/batch=0.11s
152/53700 (epoch 0.283) train_loss=215.97836304 time/batch=0.07s
153/53700 (epoch 0.285) train_loss=389.95477295 time/batch=0.11s
154/53700 (epoch 0.287) train_loss=161.34785461 time/batch=0.07s
155/53700 (epoch 0.289) train_loss=377.10198975 time/batch=0.11s
156/53700 (epoch 0.291) train_loss=155.75955200 time/batch=0.07s
157/53700 (epoch 0.292) train_loss=630.79919434 time/batch=0.17s
158/53700 (epoch 0.294) train_loss=676.55212402 time/batch=0.19s
159/53700 (epoch 0.296) train_loss=843.27874756 time/batch=0.31s
160/53700 (epoch 0.298) train_loss=319.25036621 time/batch=0.12s
161/53700 (epoch 0.300) train_loss=398.94323730 time/batch=0.11s
162/53700 (epoch 0.302) train_loss=379.77383423 time/batch=0.11s
163/53700 (epoch 0.304) train_loss=796.91369629 time/batch=0.22s
164/53700 (epoch 0.305) train_loss=470.12954712 time/batch=0.13s
165/53700 (epoch 0.307) train_loss=344.85427856 time/batch=0.12s
166/53700 (epoch 0.309) train_loss=737.91809082 time/batch=0.21s
167/53700 (epoch 0.311) train_loss=390.69573975 time/batch=0.12s
168/53700 (epoch 0.313) train_loss=478.60998535 time/batch=0.13s
169/53700 (epoch 0.315) train_loss=388.97921753 time/batch=0.12s
170/53700 (epoch 0.317) train_loss=408.19934082 time/batch=0.11s
171/53700 (epoch 0.318) train_loss=197.97784424 time/batch=0.07s
172/53700 (epoch 0.320) train_loss=411.27871704 time/batch=0.12s
173/53700 (epoch 0.322) train_loss=395.70858765 time/batch=0.12s
174/53700 (epoch 0.324) train_loss=607.36828613 time/batch=0.16s
175/53700 (epoch 0.326) train_loss=200.18112183 time/batch=0.09s
176/53700 (epoch 0.328) train_loss=158.06350708 time/batch=0.06s
177/53700 (epoch 0.330) train_loss=588.27917480 time/batch=0.15s
178/53700 (epoch 0.331) train_loss=234.98645020 time/batch=0.09s
179/53700 (epoch 0.333) train_loss=164.58755493 time/batch=0.06s
180/53700 (epoch 0.335) train_loss=233.51669312 time/batch=0.07s
181/53700 (epoch 0.337) train_loss=323.52761841 time/batch=0.10s
182/53700 (epoch 0.339) train_loss=463.98065186 time/batch=0.13s
183/53700 (epoch 0.341) train_loss=163.98425293 time/batch=0.06s
184/53700 (epoch 0.343) train_loss=604.94787598 time/batch=0.17s
185/53700 (epoch 0.345) train_loss=254.32598877 time/batch=0.08s
186/53700 (epoch 0.346) train_loss=178.34967041 time/batch=0.06s
187/53700 (epoch 0.348) train_loss=302.72592163 time/batch=0.09s
188/53700 (epoch 0.350) train_loss=369.40109253 time/batch=0.11s
189/53700 (epoch 0.352) train_loss=175.89791870 time/batch=0.06s
190/53700 (epoch 0.354) train_loss=185.45610046 time/batch=0.06s
191/53700 (epoch 0.356) train_loss=279.06774902 time/batch=0.08s
192/53700 (epoch 0.358) train_loss=265.34490967 time/batch=0.08s
193/53700 (epoch 0.359) train_loss=559.90246582 time/batch=0.16s
194/53700 (epoch 0.361) train_loss=341.05624390 time/batch=0.11s
195/53700 (epoch 0.363) train_loss=366.64752197 time/batch=0.11s
196/53700 (epoch 0.365) train_loss=623.63104248 time/batch=0.18s
197/53700 (epoch 0.367) train_loss=303.26031494 time/batch=0.11s
198/53700 (epoch 0.369) train_loss=542.39575195 time/batch=0.15s
199/53700 (epoch 0.371) train_loss=362.47021484 time/batch=0.12s
200/53700 (epoch 0.372) train_loss=419.81671143 time/batch=0.12s
201/53700 (epoch 0.374) train_loss=715.88562012 time/batch=0.20s
202/53700 (epoch 0.376) train_loss=337.04095459 time/batch=0.11s
203/53700 (epoch 0.378) train_loss=127.48378754 time/batch=0.05s
204/53700 (epoch 0.380) train_loss=168.16404724 time/batch=0.05s
205/53700 (epoch 0.382) train_loss=721.76025391 time/batch=0.20s
206/53700 (epoch 0.384) train_loss=143.20960999 time/batch=0.07s
207/53700 (epoch 0.385) train_loss=457.42138672 time/batch=0.12s
208/53700 (epoch 0.387) train_loss=345.45022583 time/batch=0.11s
209/53700 (epoch 0.389) train_loss=358.08319092 time/batch=0.11s
210/53700 (epoch 0.391) train_loss=285.98703003 time/batch=0.09s
211/53700 (epoch 0.393) train_loss=315.26455688 time/batch=0.11s
212/53700 (epoch 0.395) train_loss=395.50604248 time/batch=0.11s
213/53700 (epoch 0.397) train_loss=202.46066284 time/batch=0.07s
214/53700 (epoch 0.399) train_loss=122.14385223 time/batch=0.05s
215/53700 (epoch 0.400) train_loss=242.50320435 time/batch=0.08s
216/53700 (epoch 0.402) train_loss=474.46472168 time/batch=0.14s
217/53700 (epoch 0.404) train_loss=259.78948975 time/batch=0.07s
218/53700 (epoch 0.406) train_loss=700.29290771 time/batch=0.19s
219/53700 (epoch 0.408) train_loss=286.23944092 time/batch=0.09s
220/53700 (epoch 0.410) train_loss=231.43920898 time/batch=0.08s
221/53700 (epoch 0.412) train_loss=154.75463867 time/batch=0.06s
222/53700 (epoch 0.413) train_loss=403.91122437 time/batch=0.11s
223/53700 (epoch 0.415) train_loss=365.45697021 time/batch=0.11s
224/53700 (epoch 0.417) train_loss=152.90614319 time/batch=0.06s
225/53700 (epoch 0.419) train_loss=636.79815674 time/batch=0.17s
226/53700 (epoch 0.421) train_loss=150.58169556 time/batch=0.07s
227/53700 (epoch 0.423) train_loss=430.01290894 time/batch=0.11s
228/53700 (epoch 0.425) train_loss=386.34915161 time/batch=0.13s
229/53700 (epoch 0.426) train_loss=273.39187622 time/batch=0.10s
230/53700 (epoch 0.428) train_loss=255.71505737 time/batch=0.08s
231/53700 (epoch 0.430) train_loss=275.30035400 time/batch=0.08s
232/53700 (epoch 0.432) train_loss=311.50344849 time/batch=0.11s
233/53700 (epoch 0.434) train_loss=505.81350708 time/batch=0.14s
234/53700 (epoch 0.436) train_loss=593.08178711 time/batch=0.17s
235/53700 (epoch 0.438) train_loss=308.82815552 time/batch=0.11s
236/53700 (epoch 0.439) train_loss=528.20117188 time/batch=0.15s
237/53700 (epoch 0.441) train_loss=170.11085510 time/batch=0.07s
238/53700 (epoch 0.443) train_loss=251.82574463 time/batch=0.07s
239/53700 (epoch 0.445) train_loss=593.21533203 time/batch=0.18s
240/53700 (epoch 0.447) train_loss=501.77172852 time/batch=0.14s
241/53700 (epoch 0.449) train_loss=678.71801758 time/batch=0.19s
242/53700 (epoch 0.451) train_loss=186.92408752 time/batch=0.07s
243/53700 (epoch 0.453) train_loss=401.70846558 time/batch=0.11s
244/53700 (epoch 0.454) train_loss=542.86285400 time/batch=0.16s
245/53700 (epoch 0.456) train_loss=265.62402344 time/batch=0.09s
246/53700 (epoch 0.458) train_loss=172.92547607 time/batch=0.06s
247/53700 (epoch 0.460) train_loss=267.87261963 time/batch=0.08s
248/53700 (epoch 0.462) train_loss=377.47677612 time/batch=0.11s
249/53700 (epoch 0.464) train_loss=131.90768433 time/batch=0.05s
250/53700 (epoch 0.466) train_loss=389.85046387 time/batch=0.11s
251/53700 (epoch 0.467) train_loss=430.85617065 time/batch=0.12s
252/53700 (epoch 0.469) train_loss=197.98370361 time/batch=0.07s
253/53700 (epoch 0.471) train_loss=392.11370850 time/batch=0.11s
254/53700 (epoch 0.473) train_loss=340.31277466 time/batch=0.11s
255/53700 (epoch 0.475) train_loss=349.37927246 time/batch=0.10s
256/53700 (epoch 0.477) train_loss=502.15625000 time/batch=0.15s
257/53700 (epoch 0.479) train_loss=138.09913635 time/batch=0.05s
258/53700 (epoch 0.480) train_loss=540.48638916 time/batch=0.16s
259/53700 (epoch 0.482) train_loss=297.53616333 time/batch=0.11s
260/53700 (epoch 0.484) train_loss=403.65362549 time/batch=0.11s
261/53700 (epoch 0.486) train_loss=471.46466064 time/batch=0.13s
262/53700 (epoch 0.488) train_loss=425.17272949 time/batch=0.12s
263/53700 (epoch 0.490) train_loss=483.42346191 time/batch=0.14s
264/53700 (epoch 0.492) train_loss=417.73193359 time/batch=0.13s
265/53700 (epoch 0.493) train_loss=398.98312378 time/batch=0.12s
266/53700 (epoch 0.495) train_loss=396.54968262 time/batch=0.12s
267/53700 (epoch 0.497) train_loss=422.34741211 time/batch=0.12s
268/53700 (epoch 0.499) train_loss=225.08583069 time/batch=0.08s
269/53700 (epoch 0.501) train_loss=181.38531494 time/batch=0.07s
270/53700 (epoch 0.503) train_loss=396.74707031 time/batch=0.11s
271/53700 (epoch 0.505) train_loss=304.42672729 time/batch=0.10s
272/53700 (epoch 0.507) train_loss=475.28155518 time/batch=0.13s
273/53700 (epoch 0.508) train_loss=516.66687012 time/batch=0.15s
274/53700 (epoch 0.510) train_loss=401.66046143 time/batch=0.12s
275/53700 (epoch 0.512) train_loss=321.87365723 time/batch=0.11s
276/53700 (epoch 0.514) train_loss=210.53115845 time/batch=0.07s
277/53700 (epoch 0.516) train_loss=575.27307129 time/batch=0.15s
278/53700 (epoch 0.518) train_loss=514.50982666 time/batch=0.16s
279/53700 (epoch 0.520) train_loss=356.14321899 time/batch=0.11s
280/53700 (epoch 0.521) train_loss=515.89636230 time/batch=0.14s
281/53700 (epoch 0.523) train_loss=601.22644043 time/batch=0.20s
282/53700 (epoch 0.525) train_loss=398.15344238 time/batch=0.12s
283/53700 (epoch 0.527) train_loss=418.27258301 time/batch=0.13s
284/53700 (epoch 0.529) train_loss=450.50796509 time/batch=0.13s
285/53700 (epoch 0.531) train_loss=455.43667603 time/batch=0.14s
286/53700 (epoch 0.533) train_loss=190.92970276 time/batch=0.06s
287/53700 (epoch 0.534) train_loss=217.31637573 time/batch=0.07s
288/53700 (epoch 0.536) train_loss=213.19633484 time/batch=0.07s
289/53700 (epoch 0.538) train_loss=251.78996277 time/batch=0.08s
290/53700 (epoch 0.540) train_loss=163.97924805 time/batch=0.07s
291/53700 (epoch 0.542) train_loss=280.92327881 time/batch=0.08s
292/53700 (epoch 0.544) train_loss=377.27703857 time/batch=0.12s
293/53700 (epoch 0.546) train_loss=196.34234619 time/batch=0.06s
294/53700 (epoch 0.547) train_loss=162.10668945 time/batch=0.06s
295/53700 (epoch 0.549) train_loss=315.16348267 time/batch=0.09s
296/53700 (epoch 0.551) train_loss=155.61933899 time/batch=0.06s
297/53700 (epoch 0.553) train_loss=431.38110352 time/batch=0.11s
298/53700 (epoch 0.555) train_loss=387.89825439 time/batch=0.11s
299/53700 (epoch 0.557) train_loss=415.87548828 time/batch=0.13s
300/53700 (epoch 0.559) train_loss=414.98217773 time/batch=0.12s
301/53700 (epoch 0.561) train_loss=244.82357788 time/batch=0.09s
302/53700 (epoch 0.562) train_loss=279.32684326 time/batch=0.08s
303/53700 (epoch 0.564) train_loss=292.95159912 time/batch=0.10s
304/53700 (epoch 0.566) train_loss=359.24053955 time/batch=0.11s
305/53700 (epoch 0.568) train_loss=245.20620728 time/batch=0.08s
306/53700 (epoch 0.570) train_loss=387.72491455 time/batch=0.11s
307/53700 (epoch 0.572) train_loss=166.28741455 time/batch=0.07s
308/53700 (epoch 0.574) train_loss=675.72570801 time/batch=0.18s
309/53700 (epoch 0.575) train_loss=376.23074341 time/batch=0.12s
310/53700 (epoch 0.577) train_loss=347.68524170 time/batch=0.11s
311/53700 (epoch 0.579) train_loss=200.13249207 time/batch=0.06s
312/53700 (epoch 0.581) train_loss=506.60012817 time/batch=0.14s
313/53700 (epoch 0.583) train_loss=495.87377930 time/batch=0.15s
314/53700 (epoch 0.585) train_loss=367.16217041 time/batch=0.11s
315/53700 (epoch 0.587) train_loss=495.16351318 time/batch=0.15s
316/53700 (epoch 0.588) train_loss=429.98156738 time/batch=0.13s
317/53700 (epoch 0.590) train_loss=199.83815002 time/batch=0.07s
318/53700 (epoch 0.592) train_loss=240.29724121 time/batch=0.08s
319/53700 (epoch 0.594) train_loss=339.11459351 time/batch=0.11s
320/53700 (epoch 0.596) train_loss=446.80993652 time/batch=0.12s
321/53700 (epoch 0.598) train_loss=368.97286987 time/batch=0.12s
322/53700 (epoch 0.600) train_loss=464.23577881 time/batch=0.14s
323/53700 (epoch 0.601) train_loss=239.23133850 time/batch=0.08s
324/53700 (epoch 0.603) train_loss=257.88269043 time/batch=0.08s
325/53700 (epoch 0.605) train_loss=191.47875977 time/batch=0.07s
326/53700 (epoch 0.607) train_loss=242.66247559 time/batch=0.07s
327/53700 (epoch 0.609) train_loss=430.92242432 time/batch=0.12s
328/53700 (epoch 0.611) train_loss=194.83064270 time/batch=0.07s
329/53700 (epoch 0.613) train_loss=217.87864685 time/batch=0.07s
330/53700 (epoch 0.615) train_loss=288.08871460 time/batch=0.09s
331/53700 (epoch 0.616) train_loss=466.18212891 time/batch=0.14s
332/53700 (epoch 0.618) train_loss=492.34716797 time/batch=0.15s
333/53700 (epoch 0.620) train_loss=361.99121094 time/batch=0.11s
334/53700 (epoch 0.622) train_loss=171.85321045 time/batch=0.06s
335/53700 (epoch 0.624) train_loss=308.16546631 time/batch=0.11s
336/53700 (epoch 0.626) train_loss=312.52587891 time/batch=0.10s
337/53700 (epoch 0.628) train_loss=187.37844849 time/batch=0.06s
338/53700 (epoch 0.629) train_loss=548.66113281 time/batch=0.14s
339/53700 (epoch 0.631) train_loss=288.73083496 time/batch=0.10s
340/53700 (epoch 0.633) train_loss=559.81970215 time/batch=0.15s
341/53700 (epoch 0.635) train_loss=497.44757080 time/batch=0.15s
342/53700 (epoch 0.637) train_loss=239.06463623 time/batch=0.08s
343/53700 (epoch 0.639) train_loss=241.33462524 time/batch=0.08s
344/53700 (epoch 0.641) train_loss=244.36613464 time/batch=0.08s
345/53700 (epoch 0.642) train_loss=254.84443665 time/batch=0.08s
346/53700 (epoch 0.644) train_loss=556.67895508 time/batch=0.19s
347/53700 (epoch 0.646) train_loss=505.64245605 time/batch=0.15s
348/53700 (epoch 0.648) train_loss=374.10430908 time/batch=0.12s
349/53700 (epoch 0.650) train_loss=294.36105347 time/batch=0.08s
350/53700 (epoch 0.652) train_loss=241.38291931 time/batch=0.09s
351/53700 (epoch 0.654) train_loss=381.09179688 time/batch=0.11s
352/53700 (epoch 0.655) train_loss=359.00378418 time/batch=0.10s
353/53700 (epoch 0.657) train_loss=285.15521240 time/batch=0.09s
354/53700 (epoch 0.659) train_loss=478.44000244 time/batch=0.14s
355/53700 (epoch 0.661) train_loss=339.61334229 time/batch=0.10s
356/53700 (epoch 0.663) train_loss=505.96649170 time/batch=0.14s
357/53700 (epoch 0.665) train_loss=415.51171875 time/batch=0.12s
358/53700 (epoch 0.667) train_loss=201.66873169 time/batch=0.07s
359/53700 (epoch 0.669) train_loss=258.77020264 time/batch=0.09s
360/53700 (epoch 0.670) train_loss=207.70111084 time/batch=0.07s
361/53700 (epoch 0.672) train_loss=261.21841431 time/batch=0.09s
362/53700 (epoch 0.674) train_loss=363.50805664 time/batch=0.11s
363/53700 (epoch 0.676) train_loss=302.76217651 time/batch=0.10s
364/53700 (epoch 0.678) train_loss=230.18019104 time/batch=0.07s
365/53700 (epoch 0.680) train_loss=192.79527283 time/batch=0.07s
366/53700 (epoch 0.682) train_loss=481.36669922 time/batch=0.14s
367/53700 (epoch 0.683) train_loss=421.69921875 time/batch=0.12s
368/53700 (epoch 0.685) train_loss=188.77423096 time/batch=0.07s
369/53700 (epoch 0.687) train_loss=281.73031616 time/batch=0.08s
370/53700 (epoch 0.689) train_loss=271.35375977 time/batch=0.09s
371/53700 (epoch 0.691) train_loss=204.01739502 time/batch=0.07s
372/53700 (epoch 0.693) train_loss=257.75964355 time/batch=0.08s
373/53700 (epoch 0.695) train_loss=164.48574829 time/batch=0.06s
374/53700 (epoch 0.696) train_loss=257.98089600 time/batch=0.08s
375/53700 (epoch 0.698) train_loss=514.20251465 time/batch=0.15s
376/53700 (epoch 0.700) train_loss=461.29937744 time/batch=0.14s
377/53700 (epoch 0.702) train_loss=243.41482544 time/batch=0.08s
378/53700 (epoch 0.704) train_loss=417.17398071 time/batch=0.12s
379/53700 (epoch 0.706) train_loss=402.57128906 time/batch=0.12s
380/53700 (epoch 0.708) train_loss=417.60577393 time/batch=0.12s
381/53700 (epoch 0.709) train_loss=196.55342102 time/batch=0.06s
382/53700 (epoch 0.711) train_loss=160.58526611 time/batch=0.06s
383/53700 (epoch 0.713) train_loss=466.86798096 time/batch=0.14s
384/53700 (epoch 0.715) train_loss=332.03924561 time/batch=0.11s
385/53700 (epoch 0.717) train_loss=254.88229370 time/batch=0.08s
386/53700 (epoch 0.719) train_loss=293.49584961 time/batch=0.09s
387/53700 (epoch 0.721) train_loss=266.23907471 time/batch=0.08s
388/53700 (epoch 0.723) train_loss=465.50891113 time/batch=0.15s
389/53700 (epoch 0.724) train_loss=453.44543457 time/batch=0.13s
390/53700 (epoch 0.726) train_loss=305.07250977 time/batch=0.10s
391/53700 (epoch 0.728) train_loss=295.92813110 time/batch=0.09s
392/53700 (epoch 0.730) train_loss=462.42987061 time/batch=0.13s
393/53700 (epoch 0.732) train_loss=329.25137329 time/batch=0.11s
394/53700 (epoch 0.734) train_loss=339.66748047 time/batch=0.11s
395/53700 (epoch 0.736) train_loss=223.74971008 time/batch=0.07s
396/53700 (epoch 0.737) train_loss=208.94561768 time/batch=0.08s
397/53700 (epoch 0.739) train_loss=379.36938477 time/batch=0.11s
398/53700 (epoch 0.741) train_loss=414.13589478 time/batch=0.12s
399/53700 (epoch 0.743) train_loss=314.19894409 time/batch=0.10s
400/53700 (epoch 0.745) train_loss=260.74072266 time/batch=0.08s
401/53700 (epoch 0.747) train_loss=410.46936035 time/batch=0.12s
402/53700 (epoch 0.749) train_loss=342.93118286 time/batch=0.11s
403/53700 (epoch 0.750) train_loss=315.94131470 time/batch=0.10s
404/53700 (epoch 0.752) train_loss=199.37905884 time/batch=0.07s
405/53700 (epoch 0.754) train_loss=295.88568115 time/batch=0.09s
406/53700 (epoch 0.756) train_loss=407.21166992 time/batch=0.12s
407/53700 (epoch 0.758) train_loss=171.60552979 time/batch=0.07s
408/53700 (epoch 0.760) train_loss=254.09243774 time/batch=0.07s
409/53700 (epoch 0.762) train_loss=329.83410645 time/batch=0.11s
410/53700 (epoch 0.764) train_loss=302.35705566 time/batch=0.09s
411/53700 (epoch 0.765) train_loss=217.96304321 time/batch=0.07s
412/53700 (epoch 0.767) train_loss=334.59869385 time/batch=0.11s
413/53700 (epoch 0.769) train_loss=206.13070679 time/batch=0.07s
414/53700 (epoch 0.771) train_loss=273.47659302 time/batch=0.08s
415/53700 (epoch 0.773) train_loss=298.37438965 time/batch=0.10s
416/53700 (epoch 0.775) train_loss=218.23648071 time/batch=0.07s
417/53700 (epoch 0.777) train_loss=332.14184570 time/batch=0.11s
418/53700 (epoch 0.778) train_loss=252.82940674 time/batch=0.08s
419/53700 (epoch 0.780) train_loss=502.01190186 time/batch=0.14s
420/53700 (epoch 0.782) train_loss=212.21197510 time/batch=0.08s
421/53700 (epoch 0.784) train_loss=197.42248535 time/batch=0.07s
422/53700 (epoch 0.786) train_loss=225.49282837 time/batch=0.08s
423/53700 (epoch 0.788) train_loss=346.83782959 time/batch=0.10s
424/53700 (epoch 0.790) train_loss=282.61108398 time/batch=0.09s
425/53700 (epoch 0.791) train_loss=330.90750122 time/batch=0.09s
426/53700 (epoch 0.793) train_loss=344.32128906 time/batch=0.11s
427/53700 (epoch 0.795) train_loss=363.18688965 time/batch=0.11s
428/53700 (epoch 0.797) train_loss=291.78326416 time/batch=0.09s
429/53700 (epoch 0.799) train_loss=266.06115723 time/batch=0.09s
430/53700 (epoch 0.801) train_loss=192.63977051 time/batch=0.06s
431/53700 (epoch 0.803) train_loss=314.42620850 time/batch=0.11s
432/53700 (epoch 0.804) train_loss=271.62982178 time/batch=0.08s
433/53700 (epoch 0.806) train_loss=192.81394958 time/batch=0.06s
434/53700 (epoch 0.808) train_loss=431.56433105 time/batch=0.12s
435/53700 (epoch 0.810) train_loss=408.58422852 time/batch=0.13s
436/53700 (epoch 0.812) train_loss=480.81240845 time/batch=0.15s
437/53700 (epoch 0.814) train_loss=338.48187256 time/batch=0.11s
438/53700 (epoch 0.816) train_loss=189.82583618 time/batch=0.06s
439/53700 (epoch 0.818) train_loss=309.34802246 time/batch=0.09s
440/53700 (epoch 0.819) train_loss=268.09594727 time/batch=0.08s
441/53700 (epoch 0.821) train_loss=435.09793091 time/batch=0.13s
442/53700 (epoch 0.823) train_loss=313.26409912 time/batch=0.11s
443/53700 (epoch 0.825) train_loss=371.11340332 time/batch=0.11s
444/53700 (epoch 0.827) train_loss=231.35520935 time/batch=0.08s
445/53700 (epoch 0.829) train_loss=187.52162170 time/batch=0.07s
446/53700 (epoch 0.831) train_loss=173.80709839 time/batch=0.06s
447/53700 (epoch 0.832) train_loss=291.18212891 time/batch=0.09s
448/53700 (epoch 0.834) train_loss=163.14807129 time/batch=0.06s
449/53700 (epoch 0.836) train_loss=163.05058289 time/batch=0.07s
450/53700 (epoch 0.838) train_loss=309.58459473 time/batch=0.09s
451/53700 (epoch 0.840) train_loss=278.60519409 time/batch=0.10s
452/53700 (epoch 0.842) train_loss=215.37796021 time/batch=0.07s
453/53700 (epoch 0.844) train_loss=180.67817688 time/batch=0.07s
454/53700 (epoch 0.845) train_loss=306.54605103 time/batch=0.09s
455/53700 (epoch 0.847) train_loss=452.43920898 time/batch=0.14s
456/53700 (epoch 0.849) train_loss=299.87854004 time/batch=0.09s
457/53700 (epoch 0.851) train_loss=234.20581055 time/batch=0.08s
458/53700 (epoch 0.853) train_loss=240.55233765 time/batch=0.07s
459/53700 (epoch 0.855) train_loss=434.15966797 time/batch=0.13s
460/53700 (epoch 0.857) train_loss=219.23638916 time/batch=0.08s
461/53700 (epoch 0.858) train_loss=228.59739685 time/batch=0.07s
462/53700 (epoch 0.860) train_loss=418.64096069 time/batch=0.13s
463/53700 (epoch 0.862) train_loss=207.90179443 time/batch=0.07s
464/53700 (epoch 0.864) train_loss=242.65608215 time/batch=0.09s
465/53700 (epoch 0.866) train_loss=333.75930786 time/batch=0.11s
466/53700 (epoch 0.868) train_loss=268.00775146 time/batch=0.08s
467/53700 (epoch 0.870) train_loss=197.41961670 time/batch=0.08s
468/53700 (epoch 0.872) train_loss=463.85180664 time/batch=0.14s
469/53700 (epoch 0.873) train_loss=269.67388916 time/batch=0.10s
470/53700 (epoch 0.875) train_loss=282.79071045 time/batch=0.09s
471/53700 (epoch 0.877) train_loss=216.90759277 time/batch=0.07s
472/53700 (epoch 0.879) train_loss=285.21057129 time/batch=0.10s
473/53700 (epoch 0.881) train_loss=219.80886841 time/batch=0.07s
474/53700 (epoch 0.883) train_loss=253.34524536 time/batch=0.08s
475/53700 (epoch 0.885) train_loss=302.25128174 time/batch=0.11s
476/53700 (epoch 0.886) train_loss=253.47491455 time/batch=0.08s
477/53700 (epoch 0.888) train_loss=259.89935303 time/batch=0.09s
478/53700 (epoch 0.890) train_loss=458.00946045 time/batch=0.12s
479/53700 (epoch 0.892) train_loss=271.86047363 time/batch=0.10s
480/53700 (epoch 0.894) train_loss=299.65179443 time/batch=0.09s
481/53700 (epoch 0.896) train_loss=307.50045776 time/batch=0.09s
482/53700 (epoch 0.898) train_loss=376.12377930 time/batch=0.11s
483/53700 (epoch 0.899) train_loss=352.40365601 time/batch=0.11s
484/53700 (epoch 0.901) train_loss=374.09051514 time/batch=0.12s
485/53700 (epoch 0.903) train_loss=269.20590210 time/batch=0.08s
486/53700 (epoch 0.905) train_loss=218.98646545 time/batch=0.08s
487/53700 (epoch 0.907) train_loss=334.37686157 time/batch=0.11s
488/53700 (epoch 0.909) train_loss=344.16772461 time/batch=0.11s
489/53700 (epoch 0.911) train_loss=376.51806641 time/batch=0.11s
490/53700 (epoch 0.912) train_loss=373.48547363 time/batch=0.12s
491/53700 (epoch 0.914) train_loss=233.48831177 time/batch=0.08s
492/53700 (epoch 0.916) train_loss=210.71775818 time/batch=0.06s
493/53700 (epoch 0.918) train_loss=231.77548218 time/batch=0.08s
494/53700 (epoch 0.920) train_loss=225.08782959 time/batch=0.07s
495/53700 (epoch 0.922) train_loss=356.46322632 time/batch=0.11s
496/53700 (epoch 0.924) train_loss=318.18447876 time/batch=0.11s
497/53700 (epoch 0.926) train_loss=344.78274536 time/batch=0.11s
498/53700 (epoch 0.927) train_loss=263.19079590 time/batch=0.09s
499/53700 (epoch 0.929) train_loss=324.66671753 time/batch=0.11s
500/53700 (epoch 0.931) train_loss=324.15609741 time/batch=0.11s
501/53700 (epoch 0.933) train_loss=310.80490112 time/batch=0.11s
502/53700 (epoch 0.935) train_loss=309.17623901 time/batch=0.09s
503/53700 (epoch 0.937) train_loss=305.24401855 time/batch=0.10s
504/53700 (epoch 0.939) train_loss=368.95547485 time/batch=0.12s
505/53700 (epoch 0.940) train_loss=266.83294678 time/batch=0.09s
506/53700 (epoch 0.942) train_loss=217.08007812 time/batch=0.07s
507/53700 (epoch 0.944) train_loss=278.81445312 time/batch=0.09s
508/53700 (epoch 0.946) train_loss=271.19720459 time/batch=0.09s
509/53700 (epoch 0.948) train_loss=268.68066406 time/batch=0.10s
510/53700 (epoch 0.950) train_loss=260.31738281 time/batch=0.08s
511/53700 (epoch 0.952) train_loss=234.58448792 time/batch=0.08s
512/53700 (epoch 0.953) train_loss=323.35693359 time/batch=0.10s
513/53700 (epoch 0.955) train_loss=233.68170166 time/batch=0.08s
514/53700 (epoch 0.957) train_loss=274.71923828 time/batch=0.09s
515/53700 (epoch 0.959) train_loss=214.29013062 time/batch=0.07s
516/53700 (epoch 0.961) train_loss=279.87078857 time/batch=0.10s
517/53700 (epoch 0.963) train_loss=343.66967773 time/batch=0.10s
518/53700 (epoch 0.965) train_loss=209.15896606 time/batch=0.08s
519/53700 (epoch 0.966) train_loss=354.57601929 time/batch=0.10s
520/53700 (epoch 0.968) train_loss=291.63775635 time/batch=0.09s
521/53700 (epoch 0.970) train_loss=271.79669189 time/batch=0.08s
522/53700 (epoch 0.972) train_loss=225.19235229 time/batch=0.08s
523/53700 (epoch 0.974) train_loss=234.56967163 time/batch=0.07s
524/53700 (epoch 0.976) train_loss=298.07116699 time/batch=0.10s
525/53700 (epoch 0.978) train_loss=226.71343994 time/batch=0.08s
526/53700 (epoch 0.980) train_loss=272.66931152 time/batch=0.08s
527/53700 (epoch 0.981) train_loss=208.96224976 time/batch=0.08s
528/53700 (epoch 0.983) train_loss=235.09545898 time/batch=0.08s
529/53700 (epoch 0.985) train_loss=229.49275208 time/batch=0.08s
530/53700 (epoch 0.987) train_loss=362.39508057 time/batch=0.11s
531/53700 (epoch 0.989) train_loss=272.08050537 time/batch=0.08s
532/53700 (epoch 0.991) train_loss=310.98699951 time/batch=0.11s
533/53700 (epoch 0.993) train_loss=272.94320679 time/batch=0.09s
534/53700 (epoch 0.994) train_loss=330.83334351 time/batch=0.10s
535/53700 (epoch 0.996) train_loss=313.39532471 time/batch=0.10s
536/53700 (epoch 0.998) train_loss=328.67672729 time/batch=0.11s
537/53700 (epoch 1.000) train_loss=276.36126709 time/batch=0.11s
  saved to metadata/AddSharpAndFlat-20190201-021213.pkl
Traceback (most recent call last):
  File "train_rnn.py", line 253, in <module>
    json.dump(open(metadata_target_path+".json","w"),training_loss_list)
  File "D:\Anaconda\lib\json\__init__.py", line 189, in dump
    for chunk in iterable:
  File "D:\Anaconda\lib\json\encoder.py", line 442, in _iterencode
    o = _default(o)
  File "D:\Anaconda\lib\json\encoder.py", line 184, in default
    raise TypeError(repr(o) + " is not JSON serializable")
TypeError: <open file 'metadata/AddSharpAndFlat-20190201-021213.pkl.json', mode 'w' at 0x000000003E3DB150> is not JSON serializable
